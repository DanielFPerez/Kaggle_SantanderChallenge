{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "Hello, this notebook will gruide you through building a classifier for solving the santander customer satisfaction problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Lets start by importing dependencies\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# IMPORT\n",
    "# ===============================================================\n",
    "import include.preprocess as preprocess\n",
    "import include.printAssistant as printAssistant\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "plt.interactive(False)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Now its time to define the number of hyperparameters over which out model is going to be trained. Following options are available: \n",
    "<ul>\n",
    "    <li>n_pca_dimension_cases:  number of possible dimensions when applying PCA</li>\n",
    "    <li>n_n_estimators: number of possible \"n_estimators\", i.e. number of used trees, for a random forest model </li>\n",
    "    <li>n_max_depth: maximum depth of a tree for a random forest classifier\n",
    "</ul> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pca_dimension_cases = 2\n",
    "n_n_estimators = 2\n",
    "n_max_depth = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Other values include lists, whose elements have to be changed manually. The possible values are found in the module \"include/preprocess.py\". These lists include:\n",
    "<ul>\n",
    "    <li>df_filter_cases: different variants of feature selection for training the model</li>\n",
    "    <li>norm_case_cases: possible data normalization alternatives</li>\n",
    "</ul> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cases extracting different features from the available data\n",
    "# See include/preprocess.py for more df_filter_cases options\n",
    "df_filter_cases = ['low-valued-plus', 'middle-valued-plus', 'high-valued']\n",
    "\n",
    "# Set normalization case implemented\n",
    "norm_case_cases = ['mean', 'minmax']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>: select previous variables according to the computational power available.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Lets finish setting up the variables for performing the hyper-parameter search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of PCA components to try out\n",
    "# Choose 3 random values between 3 dimensions and 20 dimensions\n",
    "pca_dimension_cases = np.random.randint(low=3, high=18, size=n_pca_dimension_cases)\n",
    "# Append value 15 as was the highest performing PCA dimension during manual exploration\n",
    "if 15 not in pca_dimension_cases:\n",
    "    pca_dimension_cases = np.append(pca_dimension_cases, 15)\n",
    "\n",
    "# Parameters for the random forest\n",
    "# Number of trees in the RF\n",
    "n_estimators = np.random.randint(2, 15, size=n_n_estimators)\n",
    "if 10 not in n_estimators:\n",
    "    n_estimators = np.append(n_estimators, 10)\n",
    "# Max depth of tree\n",
    "max_depth = np.random.randint(5, 40, size=n_max_depth)\n",
    "if 30 not in n_estimators:\n",
    "    max_depth = np.append(max_depth, 30)\n",
    "\n",
    "rf_grid = [(i, j) for i in n_estimators.tolist() for j in max_depth.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Lets see how many possible cases are we going to evaluate:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of  108  cases will be evaluated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The firs \"4*2\" multiplication comes from the df_filter_cases and norm_case_cases lists (see below) \n",
    "n_tot_cases = n_tot_cases = len(norm_case_cases) * len(df_filter_cases) * pca_dimension_cases.shape[0] * n_estimators.shape[0] * max_depth.shape[0]\n",
    "print(\"A total of \", n_tot_cases, \" cases will be evaluated.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Now we define variables for logging and saving the optimal model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for storing best performance tree in the test set\n",
    "rf_winner = {\"logger_index\": None,\n",
    "             \"filter_case\": None,\n",
    "             \"norm_case\": None,\n",
    "             \"pca_dim\": 0,\n",
    "             \"n_trees\": 0,\n",
    "             \"tree_depth\": 0,\n",
    "             \"auc_val\": 0.0,\n",
    "             \"auc_test\": 0.0,\n",
    "             \"rf_object\": None}\n",
    "roc_auc_test_tracker = 0.0\n",
    "roc_auc_val_tracker = 0.0\n",
    "\n",
    "# Pandas dataFrame for logging\n",
    "out_cols = [\"filter_case\", \"norm_case\", \"pca_dim\", \"n_trees\", \"tree_depth\", \"cv_mean_auc\", \"test_auc\", \"image_path\"]\n",
    "pd_logger = pd.DataFrame(columns=out_cols)\n",
    "logger_index = 0\n",
    "\n",
    "# Directory for saving winner\n",
    "out_path_winner = \"./data/output/\"\n",
    "\n",
    "# Directory for saving logger and plots\n",
    "out_path_log = \"./data/output/log/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Search the optimal model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Build-Train-Test\n",
    "# ===============================================================\n",
    "\n",
    "# Load a pre-cleaned the dataset\n",
    "df_raw = preprocess.get_raw_cleaned_data()\n",
    "\n",
    "# Iterate over filter cases\n",
    "for feature_filter in df_filter_cases:\n",
    "    # Subtract a subset of the feature columns from filtered dataset\n",
    "    df_filtered = preprocess.filter_dataframe(dataframe=df_raw, filter_case=feature_filter)\n",
    "\n",
    "    for norm_case in norm_case_cases:\n",
    "        df_normalized = preprocess.norm_transform_dataframe(dataframe=df_filtered, norm_case=norm_case,\n",
    "                                                            scale_factor=1000)\n",
    "\n",
    "        for pca_dim in pca_dimension_cases:\n",
    "            # Apply PCA\n",
    "            df_pca = preprocess.apply_pca(dataframe=df_normalized, dimensions=pca_dim)\n",
    "\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = preprocess.split_datagram_test_train(df_pca,\n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "            # Perform oversampling\n",
    "            X_smot, y_smot = preprocess.perform_smote(X_train, y_train)\n",
    "\n",
    "            for (estim, depth) in rf_grid:\n",
    "                cv = StratifiedKFold(n_splits=8)\n",
    "                classifier = RandomForestClassifier(n_estimators=estim, max_depth=depth, min_samples_split=600)\n",
    "\n",
    "                tprs = []\n",
    "                aucs = []\n",
    "                mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "                cv_tracker = 0\n",
    "                for train, test in cv.split(X_smot, y_smot):\n",
    "                    classifier.fit(X_smot[train], y_smot[train])\n",
    "                    probabilities = classifier.predict_proba(X_smot[test])\n",
    "                    # Compute ROC curve and area the curve\n",
    "                    fpr, tpr, thresholds = roc_curve(y_smot[test], probabilities[:,1])\n",
    "                    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "                    tprs[-1][0] = 0.0\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    aucs.append(roc_auc)\n",
    "                    # Add result of cv fold to the final auc plot\n",
    "                    printAssistant.subplot_rocauc_cvfold(fpr, tpr, roc_auc, cv_tracker)\n",
    "                    # Increment cv tracker\n",
    "                    cv_tracker += 1\n",
    "\n",
    "                mean_tpr = np.mean(tprs, axis=0)\n",
    "                mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                std_auc = np.std(aucs)\n",
    "                std_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "                # Evaluate on test set\n",
    "                probs_test = classifier.predict_proba(X_test)\n",
    "                fpr_test, tpr_test, thresholds_test = roc_curve(y_test, probs_test[:, 1])\n",
    "                roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "                # Log result if new best AUC over x-validation found\n",
    "                if mean_auc > roc_auc_val_tracker:\n",
    "                    roc_auc_val_tracker = mean_auc\n",
    "                    print(\"Maximum cv mean auc found: \", mean_auc, \" at logger index: \", logger_index)\n",
    "                    printAssistant.plot_total_rocauc(mean_tpr, mean_auc, std_auc, std_tpr, mean_fpr)\n",
    "                    img_path = out_path_log + \"cv_roc_plot_\" + str(logger_index) + \".png\"\n",
    "                    printAssistant.save_auc_roc_plot(img_path)\n",
    "                    pd_logger.loc[logger_index] = {\"filter_case\": feature_filter,\n",
    "                                                   \"norm_case\": norm_case,\n",
    "                                                   \"pca_dim\": pca_dim,\n",
    "                                                   \"n_trees\": estim,\n",
    "                                                   \"tree_depth\": depth,\n",
    "                                                   \"cv_mean_auc\": mean_auc,\n",
    "                                                   \"test_auc\": roc_auc_test,\n",
    "                                                   \"image_path\": img_path}\n",
    "                    # Close plot to be able to plot after each iteration\n",
    "                    printAssistant.close_plot()\n",
    "\n",
    "                # Log winner rf if test performance achieved\n",
    "                if roc_auc_test > roc_auc_test_tracker:\n",
    "                    roc_auc_test_tracker = roc_auc_test\n",
    "                    rf_winner[\"logger_index\"] = logger_index\n",
    "                    rf_winner[\"filter_case\"] = feature_filter\n",
    "                    rf_winner[\"pca_dim\"] = pca_dim\n",
    "                    rf_winner[\"n_trees\"] = estim\n",
    "                    rf_winner[\"tree_depth\"] = depth\n",
    "                    rf_winner[\"auc_val\"] = mean_auc\n",
    "                    rf_winner[\"auc_test\"] = roc_auc_test\n",
    "                    rf_winner[\"rf_object\"] = classifier\n",
    "                    print(\"New best AUC over test set found: \", roc_auc_test, \"at logger index: \", logger_index)\n",
    "                    # Add case to log file\n",
    "                    printAssistant.plot_total_rocauc(mean_tpr, mean_auc, std_auc, std_tpr, mean_fpr)\n",
    "                    img_path = out_path_log + \"cv_roc_plot_\" + str(logger_index) + \".png\"\n",
    "                    printAssistant.save_auc_roc_plot(img_path)\n",
    "                    pd_logger.loc[logger_index] = {\"filter_case\": feature_filter,\n",
    "                                                   \"norm_case\": norm_case,\n",
    "                                                   \"pca_dim\": pca_dim,\n",
    "                                                   \"n_trees\": estim,\n",
    "                                                   \"tree_depth\": depth,\n",
    "                                                   \"cv_mean_auc\": mean_auc,\n",
    "                                                   \"test_auc\": roc_auc_test,\n",
    "                                                   \"image_path\": img_path}\n",
    "                    # Save/overwrite image of rf_winner\n",
    "                    printAssistant.save_auc_roc_plot(out_path_winner+\"rf_winner.png\")\n",
    "                    # Close plot to be able to plot after each iteration\n",
    "                    printAssistant.close_plot()\n",
    "\n",
    "                # Increment the logger index\n",
    "                print(\"######## Model iteration \", logger_index, \" finished ################################ \")\n",
    "                logger_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Save log file and optimal results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save log table\n",
    "pd_logger.to_csv(out_path_log+\"log.csv\")\n",
    "\n",
    "# Save winner model\n",
    "pickle_out = out_path_winner+\"rf_winner.pkl\"\n",
    "with open(pickle_out, 'wb') as file:\n",
    "    pickle.dump(rf_winner, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model is trained!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
